#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
FasterWhisperä¸Pyannoteé›†æˆçš„è¯­éŸ³è½¬æ–‡å­—å·¥å…·

ä¸€ä¸ªé«˜æ•ˆã€ç¨³å®šçš„è¯­éŸ³è½¬æ–‡å­—å‘½ä»¤è¡Œå·¥å…·ï¼ŒåŸºäºFaster Whisperå’ŒPyannoteæ¨¡å‹ã€‚
æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼ã€æ‰¹å¤„ç†ã€è¯´è¯äººåˆ†å‰²å’Œç²¾ç¡®çš„è¿›åº¦æŠ¥å‘Šã€‚
å¢å¼ºç‰ˆï¼šæ”¯æŒm4aæ ¼å¼è‡ªåŠ¨è½¬æ¢ä¸ºwavä»¥é€‚é…è¯´è¯äººåˆ†å‰²
"""

import os
import sys
import time
import logging
import argparse
import json
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, List, Any, Union, Generator, Tuple, Iterator, Set
import io
import numpy as np

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("transcribe.log", encoding="utf-8"),
    ]
)
logger = logging.getLogger("whisper_transcriber")

# =========================== ç¯å¢ƒé…ç½® ===========================
# é»˜è®¤ä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡
CUDA_PATH = None     # CUDAå®‰è£…è·¯å¾„
CUDNN_PATH = None    # cuDNNåº“è·¯å¾„
FFMPEG_PATH = None   # FFmpegå¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„

# è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨æ·»åŠ è·¯å¾„åˆ°ç¯å¢ƒå˜é‡
def add_path_to_env(path, name):
    if path and Path(path).exists():
        path_sep = ";" if os.name == "nt" else ":"
        os.environ["PATH"] = path + path_sep + os.environ.get("PATH", "")
        logger.info(f"ä½¿ç”¨è‡ªå®šä¹‰{name}è·¯å¾„: {path}")
        return True
    elif path:
        logger.warning(f"æŒ‡å®šçš„{name}è·¯å¾„ä¸å­˜åœ¨: {path}")
    return False

# é˜²æ­¢OpenMPå¤šæ¬¡åŠ è½½é—®é¢˜
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
# é™åˆ¶OpenMPçº¿ç¨‹æ•°ï¼Œé¿å…è¿‡åº¦ä½¿ç”¨CPU
os.environ["OMP_NUM_THREADS"] = "4"
# =================================================================

# ä¸´æ—¶æ–‡ä»¶åˆ—è¡¨ï¼Œç”¨äºè·Ÿè¸ªéœ€è¦æ¸…ç†çš„æ–‡ä»¶
TEMP_FILES = []

# è®¡æ—¶è£…é¥°å™¨
def timer(func):
    """æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´çš„è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        elapsed_time = time.time() - start_time
        logger.info(f"å‡½æ•° {func.__name__} æ‰§è¡Œè€—æ—¶: {elapsed_time:.2f}s")
        return result
    return wrapper

class SafeGPUDetector:
    """å®‰å…¨GPUæ£€æµ‹å™¨ - é˜²å´©æºƒè®¾è®¡"""
    
    @staticmethod
    def check_cuda_available() -> Dict[str, Any]:
        """åˆ†å±‚å®‰å…¨GPUæ£€æµ‹ - æ°¸ä¸å´©æºƒ"""
        
        status = {
            "cuda_available": False,
            "device": "cpu", 
            "compute_type": "float32",
            "gpu_info": None,
            "detection_method": "å®‰å…¨æ£€æµ‹",
            "error_details": [],
            "fallback_reason": None
        }
        
        # ğŸ›¡ï¸ ç¬¬ä¸€å±‚ï¼šPyTorchå¯¼å…¥å’ŒåŸºç¡€æ£€æŸ¥
        try:
            import torch
            if not torch.cuda.is_available():
                status["fallback_reason"] = "CUDAç¼–è¯‘æ”¯æŒä¸å¯ç”¨"
                logger.info("GPUæ£€æµ‹: CUDAç¼–è¯‘æ”¯æŒä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡å¼")
                return status
        except Exception as e:
            status["error_details"].append(f"PyTorchå¯¼å…¥å¤±è´¥: {e}")
            status["fallback_reason"] = "PyTorchç¯å¢ƒé—®é¢˜"
            logger.warning(f"GPUæ£€æµ‹: PyTorchå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨CPUæ¨¡å¼: {e}")
            return status
        
        # ğŸ›¡ï¸ ç¬¬äºŒå±‚ï¼šè®¾å¤‡è®¡æ•°å®‰å…¨æ£€æŸ¥
        try:
            device_count = torch.cuda.device_count()
            if device_count == 0:
                status["fallback_reason"] = "æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡"
                logger.info("GPUæ£€æµ‹: æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œä½¿ç”¨CPUæ¨¡å¼")
                return status
        except Exception as e:
            status["error_details"].append(f"è®¾å¤‡è®¡æ•°å¤±è´¥: {e}")
            status["fallback_reason"] = "GPUæšä¸¾å¤±è´¥"
            logger.warning(f"GPUæ£€æµ‹: è®¾å¤‡æšä¸¾å¤±è´¥ï¼Œä½¿ç”¨CPUæ¨¡å¼: {e}")
            return status
        
        # ğŸ›¡ï¸ ç¬¬ä¸‰å±‚ï¼šè®¾å¤‡è®¿é—®å®‰å…¨æµ‹è¯•
        try:
            # å®‰å…¨çš„å½“å‰è®¾å¤‡æ£€æŸ¥
            current_device = torch.cuda.current_device()
            device_name = torch.cuda.get_device_name(current_device)
            
        except RuntimeError as e:
            status["error_details"].append(f"è®¾å¤‡è®¿é—®å¤±è´¥: {e}")
            status["fallback_reason"] = "GPUè®¾å¤‡ä¸å¯è®¿é—®"
            logger.warning(f"GPUæ£€æµ‹: è®¾å¤‡è®¿é—®å¤±è´¥ï¼Œä½¿ç”¨CPUæ¨¡å¼: {e}")
            return status
        except Exception as e:
            status["error_details"].append(f"è®¾å¤‡ä¿¡æ¯è·å–å¤±è´¥: {e}")
            status["fallback_reason"] = "GPUçŠ¶æ€å¼‚å¸¸"
            logger.warning(f"GPUæ£€æµ‹: è®¾å¤‡çŠ¶æ€å¼‚å¸¸ï¼Œä½¿ç”¨CPUæ¨¡å¼: {e}")
            return status
        
        # ğŸ›¡ï¸ ç¬¬å››å±‚ï¼šå†…å­˜å’Œèƒ½åŠ›å®‰å…¨æµ‹è¯•
        try:
            # æµ‹è¯•åŸºæœ¬GPUæ“ä½œ
            test_tensor = torch.randn(10, 10, device='cuda')
            _ = test_tensor + 1  # åŸºæœ¬è¿ç®—æµ‹è¯•
            del test_tensor
            torch.cuda.empty_cache()
            
            # è·å–è®¾å¤‡è¯¦ç»†ä¿¡æ¯
            props = torch.cuda.get_device_properties(current_device)
            capability = torch.cuda.get_device_capability(current_device)
            
            # âœ… æˆåŠŸæ£€æµ‹ï¼Œè®¾ç½®GPUçŠ¶æ€
            status.update({
                "cuda_available": True,
                "device": "cuda",
                "compute_type": "float16" if capability[0] >= 7 else "int8",
                "gpu_info": {
                    "name": device_name,
                    "total_memory_GB": round(props.total_memory / (1024**3), 2),
                    "cuda_version": torch.version.cuda,
                    "device_id": current_device,
                    "capability": f"{capability[0]}.{capability[1]}",
                    "multi_processor_count": props.multi_processor_count
                }
            })
            
            logger.info(f"GPUæ£€æµ‹æˆåŠŸ: {device_name}, "
                       f"å†…å­˜: {status['gpu_info']['total_memory_GB']}GB, "
                       f"è®¡ç®—ç±»å‹: {status['compute_type']}")
            
        except torch.cuda.OutOfMemoryError as e:
            status["error_details"].append("GPUå†…å­˜ä¸è¶³")
            status["fallback_reason"] = "GPUå†…å­˜è€—å°½"
            logger.warning(f"GPUæ£€æµ‹: å†…å­˜ä¸è¶³ï¼Œä½¿ç”¨CPUæ¨¡å¼: {e}")
        except Exception as e:
            status["error_details"].append(f"GPUåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            status["fallback_reason"] = "GPUåŠŸèƒ½å¼‚å¸¸"
            logger.warning(f"GPUæ£€æµ‹: åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œä½¿ç”¨CPUæ¨¡å¼: {e}")
        
        return status
    
    @staticmethod
    def get_safe_gpu_memory_usage() -> Dict[int, Dict[str, float]]:
        """å®‰å…¨çš„GPUå†…å­˜ä½¿ç”¨ç‡æ£€æŸ¥"""
        try:
            import torch
            if not torch.cuda.is_available():
                return {}
                
            gpu_memory_usage = {}
            device_count = torch.cuda.device_count()
            
            for i in range(device_count):
                try:
                    total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                    allocated = torch.cuda.memory_allocated(i) / 1024**3
                    reserved = torch.cuda.memory_reserved(i) / 1024**3
                    
                    gpu_memory_usage[i] = {
                        "total_GB": round(total_memory, 2),
                        "used_GB": round(allocated, 2), 
                        "reserved_GB": round(reserved, 2),
                        "free_GB": round(total_memory - allocated, 2),
                        "usage_percent": round(allocated / total_memory * 100, 2)
                    }
                except Exception as e:
                    # å•ä¸ªGPUæ£€æŸ¥å¤±è´¥ä¸å½±å“å…¶ä»–GPU
                    logger.debug(f"GPU {i} å†…å­˜æ£€æŸ¥å¤±è´¥: {e}")
                    continue
                    
            return gpu_memory_usage
            
        except Exception as e:
            logger.debug(f"GPUå†…å­˜ä½¿ç”¨ç‡æ£€æŸ¥å¤±è´¥: {e}")
            return {}


class SpeakerDiarization:
    """è¯´è¯äººåˆ†å‰²å¤„ç†ç±»"""
    
    def __init__(self, device: str = "auto", model_path: str = None):
        """
        åˆå§‹åŒ–è¯´è¯äººåˆ†å‰²å¤„ç†å™¨
        
        Args:
            device: è®¡ç®—è®¾å¤‡ ("cuda", "cpu", "auto")
            model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„æˆ–HuggingFaceæ¨¡å‹ID
        """
        # æ£€æµ‹CUDAå¯ç”¨æ€§
        gpu_status = SafeGPUDetector.check_cuda_available()
        self.device = device
        if device == "auto":
            self.device = gpu_status["device"]
        
        # éªŒè¯æ¨¡å‹è·¯å¾„
        if model_path is None:
            raise ValueError("å¿…é¡»æä¾›æœ‰æ•ˆçš„æ¨¡å‹è·¯å¾„æˆ–Hugging Faceæ¨¡å‹ID")
        
        self.model_path = model_path
        self._pipeline = None
        self._memory_cache = {}  # ç”¨äºç¼“å­˜éŸ³é¢‘æ–‡ä»¶æ•°æ®
        
        logger.info(f"è¯´è¯äººåˆ†å‰²å°†ä½¿ç”¨è®¾å¤‡: {self.device}")
        logger.info(f"å°†ä½¿ç”¨æ¨¡å‹: {model_path}")
    
    @property
    def pipeline(self):
        """å»¶è¿ŸåŠ è½½åˆ†å‰²æ¨¡å‹"""
        if self._pipeline is None:
            try:
                from pyannote.audio import Pipeline
                import torch
                import os
                import yaml
                
                logger.info("åŠ è½½è¯´è¯äººåˆ†å‰²æ¨¡å‹...")
                load_start = time.time()
                
                # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                config_path = os.path.join(self.model_path, "config.yaml")
                
                # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»º
                if not os.path.exists(config_path):
                    logger.info(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»º: {config_path}")
                    
                    # æ£€æŸ¥æ‰€éœ€æ¨¡å‹æ–‡ä»¶è·¯å¾„
                    base_dir = os.path.dirname(self.model_path)
                    embedding_model_path = os.path.join(base_dir, "wespeaker-voxceleb-resnet34-LM", "pytorch_model.bin")
                    segmentation_model_path = os.path.join(base_dir, "segmentation-3.0", "pytorch_model.bin")
                    
                    # å¦‚æœç›®å½•ç»“æ„ä¸ä¸€è‡´ï¼Œå°è¯•åœ¨æ¨¡å‹è·¯å¾„å†…éƒ¨å¯»æ‰¾
                    if not os.path.exists(embedding_model_path):
                        embedding_model_path = os.path.join(self.model_path, "wespeaker-voxceleb-resnet34-LM", "pytorch_model.bin")
                    
                    if not os.path.exists(segmentation_model_path):
                        segmentation_model_path = os.path.join(self.model_path, "segmentation-3.0", "pytorch_model.bin")
                    
                    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    if not os.path.exists(embedding_model_path):
                        logger.error(f"åµŒå…¥æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {embedding_model_path}")
                        logger.info("è¯·ç¡®ä¿æ¨¡å‹ç›®å½•ç»“æ„å¦‚ä¸‹:")
                        logger.info(f"  {base_dir}/")
                        logger.info(f"  â”œâ”€â”€ speaker-diarization-3.1/")
                        logger.info(f"  â”œâ”€â”€ wespeaker-voxceleb-resnet34-LM/pytorch_model.bin")
                        logger.info(f"  â””â”€â”€ segmentation-3.0/pytorch_model.bin")
                        raise FileNotFoundError(f"åµŒå…¥æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {embedding_model_path}")
                    
                    if not os.path.exists(segmentation_model_path):
                        logger.error(f"åˆ†å‰²æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {segmentation_model_path}")
                        raise FileNotFoundError(f"åˆ†å‰²æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {segmentation_model_path}")
                    
                    # åˆ›å»ºconfig.yamlå†…å®¹
                    config = {
                        "version": "3.1.0",
                        "pipeline": {
                            "name": "pyannote.audio.pipelines.SpeakerDiarization",
                            "params": {
                                "clustering": "AgglomerativeClustering",
                                "embedding": embedding_model_path,
                                "embedding_batch_size": 32,
                                "embedding_exclude_overlap": True,
                                "segmentation": segmentation_model_path,
                                "segmentation_batch_size": 32
                            }
                        },
                        "params": {
                            "clustering": {
                                "method": "centroid",
                                "min_cluster_size": 12,
                                "threshold": 0.7045654963945799
                            },
                            "segmentation": {
                                "min_duration_off": 0.0
                            }
                        }
                    }
                    
                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                    os.makedirs(os.path.dirname(config_path), exist_ok=True)
                    
                    # å†™å…¥é…ç½®æ–‡ä»¶
                    with open(config_path, 'w', encoding='utf-8') as f:
                        yaml.dump(config, f, default_flow_style=False)
                    
                    logger.info(f"æˆåŠŸåˆ›å»ºé…ç½®æ–‡ä»¶: {config_path}")
                
                # ç°åœ¨åŠ è½½æ¨¡å‹
                logger.info(f"ä»é…ç½®æ–‡ä»¶åŠ è½½è¯´è¯äººåˆ†å‰²æ¨¡å‹: {config_path}")
                self._pipeline = Pipeline.from_pretrained(config_path)
                
                # ç§»åŠ¨åˆ°é€‚å½“çš„è®¾å¤‡
                self._pipeline.to(torch.device(self.device))
                
                load_time = time.time() - load_start
                logger.info(f"è¯´è¯äººåˆ†å‰²æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}s")
                
            except ImportError as e:
                logger.error(f"å¯¼å…¥ pyannote.audio å¤±è´¥: {e}")
                logger.error("è¯·ç¡®ä¿å·²å®‰è£… pyannote.audio: pip install pyannote.audio")
                logger.error("å¯èƒ½è¿˜éœ€è¦ PyYAML: pip install pyyaml")
                raise
            except Exception as e:
                logger.error(f"è¯´è¯äººåˆ†å‰²æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                logger.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
                
                # è¯Šæ–­ä¿¡æ¯
                if "not installed" in str(e) or "No module named" in str(e):
                    logger.error("ç¼ºå°‘ä¾èµ–åº“ã€‚è¯·ç¡®ä¿å·²å®‰è£…: pip install pyannote.audio torch pyyaml")
                elif "yaml" in str(e).lower():
                    logger.error("YAMLç›¸å…³é”™è¯¯ã€‚è¯·å®‰è£…PyYAML: pip install pyyaml")
                
                raise
        
        return self._pipeline
    
    def _preload_audio(self, audio_file: str) -> None:
        """é¢„åŠ è½½éŸ³é¢‘åˆ°å†…å­˜ç¼“å­˜"""
        if audio_file not in self._memory_cache:
            try:
                import soundfile as sf
                logger.debug(f"é¢„åŠ è½½éŸ³é¢‘åˆ°å†…å­˜: {audio_file}")
                # ä½¿ç”¨soundfileåŠ è½½éŸ³é¢‘æ•°æ®
                data, sample_rate = sf.read(audio_file)
                self._memory_cache[audio_file] = {
                    "data": data,
                    "sample_rate": sample_rate
                }
                logger.debug(f"éŸ³é¢‘é¢„åŠ è½½å®Œæˆï¼Œæ ·æœ¬ç‡: {sample_rate}Hz, æ•°æ®å¤§å°: {data.shape}")
            except Exception as e:
                logger.warning(f"éŸ³é¢‘é¢„åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨ç›´æ¥æ–‡ä»¶åŠ è½½: {e}")
    
    def _clear_cache(self, audio_file: Optional[str] = None):
        """æ¸…ç†å†…å­˜ç¼“å­˜"""
        if audio_file:
            if audio_file in self._memory_cache:
                del self._memory_cache[audio_file]
                logger.debug(f"å·²æ¸…ç†éŸ³é¢‘ç¼“å­˜: {audio_file}")
        else:
            self._memory_cache.clear()
            logger.debug("å·²æ¸…ç†æ‰€æœ‰éŸ³é¢‘ç¼“å­˜")
    
    @timer
    def process(
        self, 
        audio_file: str,
        num_speakers: Optional[int] = None,
        min_speakers: Optional[int] = None,
        max_speakers: Optional[int] = None,
        preload_audio: bool = True
    ) -> Dict[str, Any]:
        """
        å¤„ç†éŸ³é¢‘æ–‡ä»¶å¹¶è¿›è¡Œè¯´è¯äººåˆ†å‰²
        
        Args:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            num_speakers: æŒ‡å®šè¯´è¯äººæ•°é‡ï¼ˆå¦‚æœå·²çŸ¥ï¼‰
            min_speakers: æœ€å°è¯´è¯äººæ•°é‡
            max_speakers: æœ€å¤§è¯´è¯äººæ•°é‡
            preload_audio: æ˜¯å¦é¢„åŠ è½½éŸ³é¢‘åˆ°å†…å­˜
            
        Returns:
            è¯´è¯äººåˆ†å‰²ç»“æœ
        """
        logger.info(f"å¼€å§‹è¯´è¯äººåˆ†å‰²: {audio_file}")
        
        # å¯é€‰ï¼šé¢„åŠ è½½éŸ³é¢‘åˆ°å†…å­˜
        if preload_audio:
            self._preload_audio(audio_file)
        
        # æ„å»ºåˆ†å‰²å‚æ•°
        diarization_options = {}
        if num_speakers is not None:
            diarization_options["num_speakers"] = num_speakers
        else:
            if min_speakers is not None:
                diarization_options["min_speakers"] = min_speakers
            if max_speakers is not None:
                diarization_options["max_speakers"] = max_speakers
        
        try:
            # æ‰§è¡Œåˆ†å‰²
            diarization = self.pipeline(audio_file, **diarization_options)
            
            # è½¬æ¢ç»“æœä¸ºæ˜“äºä½¿ç”¨çš„æ ¼å¼
            result = {"speakers": {}}
            
            # å°†PyAnnoteæ—¶é—´è½´ç»“æœè½¬æ¢ä¸ºæ—¶é—´ç‰‡æ®µåˆ—è¡¨
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                start_time = turn.start
                end_time = turn.end
                
                if speaker not in result["speakers"]:
                    result["speakers"][speaker] = []
                
                result["speakers"][speaker].append({
                    "start": start_time,
                    "end": end_time
                })
            
            # æ·»åŠ ä¸€ä¸ªå±•å¹³çš„æ—¶é—´è½´è§†å›¾ï¼ˆæŒ‰æ—¶é—´æ’åºçš„æ‰€æœ‰ç‰‡æ®µï¼‰
            timeline = []
            for speaker, segments in result["speakers"].items():
                for segment in segments:
                    timeline.append({
                        "start": segment["start"],
                        "end": segment["end"],
                        "speaker": speaker
                    })
            
            # æŒ‰å¼€å§‹æ—¶é—´æ’åº
            timeline.sort(key=lambda x: x["start"])
            result["timeline"] = timeline
            
            # è®¡ç®—æ¯ä¸ªè¯´è¯äººçš„è¯´è¯æ—¶é•¿å’Œç™¾åˆ†æ¯”
            total_duration = sum(seg["end"] - seg["start"] for seg in timeline)
            speaker_stats = {}
            
            for speaker, segments in result["speakers"].items():
                speaker_duration = sum(seg["end"] - seg["start"] for seg in segments)
                speaker_stats[speaker] = {
                    "duration": speaker_duration,
                    "percentage": (speaker_duration / total_duration * 100) if total_duration > 0 else 0
                }
            
            result["speaker_stats"] = speaker_stats
            result["total_duration"] = total_duration
            result["num_speakers"] = len(result["speakers"])
            
            # æ¸…ç†ç¼“å­˜
            if preload_audio:
                self._clear_cache(audio_file)
            
            return result
            
        except Exception as e:
            logger.error(f"è¯´è¯äººåˆ†å‰²å¤„ç†å¤±è´¥: {e}")
            # æ¸…ç†ç¼“å­˜
            if preload_audio:
                self._clear_cache(audio_file)
            raise
    
    def close(self):
        """é‡Šæ”¾æ¨¡å‹èµ„æº"""
        if self._pipeline is not None:
            del self._pipeline
            self._pipeline = None
        self._clear_cache()

class TranscriptionFormatter:
    """è½¬å½•ç»“æœæ ¼å¼åŒ–å·¥å…·ç±»"""
    
    @staticmethod
    def format_srt(segments: List[Any], output_file: str) -> None:
        """å°†è½¬å½•ç»“æœä¿å­˜ä¸ºSRTå­—å¹•æ ¼å¼"""
        with open(output_file, "w", encoding="utf-8") as f:
            for i, segment in enumerate(segments, 1):
                # è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼ (HH:MM:SS,mmm)
                start_time = TranscriptionFormatter._format_timestamp(segment.start)
                end_time = TranscriptionFormatter._format_timestamp(segment.end)
                
                f.write(f"{i}\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{segment.text.strip()}\n\n")
                f.flush()
    
    @staticmethod
    def format_json(segments: List[Any], info: Any, output_file: str) -> None:
        """å°†è½¬å½•ç»“æœä¿å­˜ä¸ºJSONæ ¼å¼"""
        result = {
            "language": info.language,
            "language_probability": info.language_probability,
            "segments": []
        }
        
        for segment in segments:
            result["segments"].append({
                "start": segment.start,
                "end": segment.end,
                "text": segment.text.strip()
            })
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
    
    @staticmethod
    def _format_timestamp(seconds: float) -> str:
        """å°†ç§’è½¬æ¢ä¸ºSRTæ ¼å¼æ—¶é—´æˆ³ (HH:MM:SS,mmm)"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = seconds % 60
        return f"{hours:02d}:{minutes:02d}:{secs:06.3f}".replace(".", ",")
    
    @staticmethod
    def format_srt_with_speakers(segments: List[Any], diarization_result: Dict[str, Any], output_file: str) -> None:
        """å°†è½¬å½•ç»“æœå’Œè¯´è¯äººåˆ†å‰²ç»“æœä¿å­˜ä¸ºå¸¦è¯´è¯äººæ ‡ç­¾çš„SRTå­—å¹•æ ¼å¼"""
        # å°†è½¬å½•æ®µè½ä¸è¯´è¯äººæ—¶é—´è½´å¯¹é½
        segments_with_speaker = []
        
        for segment in segments:
            # æ‰¾å‡ºä¸å½“å‰ç‰‡æ®µé‡å çš„è¯´è¯äºº
            segment_start = segment.start
            segment_end = segment.end
            
            # å¯»æ‰¾é‡å æœ€å¤šçš„è¯´è¯äºº
            best_speaker = None
            max_overlap = 0
            
            for timeline_item in diarization_result["timeline"]:
                speaker_start = timeline_item["start"]
                speaker_end = timeline_item["end"]
                
                # è®¡ç®—é‡å 
                overlap_start = max(segment_start, speaker_start)
                overlap_end = min(segment_end, speaker_end)
                overlap = max(0, overlap_end - overlap_start)
                
                if overlap > max_overlap:
                    max_overlap = overlap
                    best_speaker = timeline_item["speaker"]
            
            # æ·»åŠ è¯´è¯äººä¿¡æ¯
            segments_with_speaker.append({
                "start": segment_start,
                "end": segment_end,
                "text": segment.text.strip(),
                "speaker": best_speaker if max_overlap > 0 else "æœªçŸ¥"
            })
        
        # å†™å…¥SRTæ–‡ä»¶
        with open(output_file, "w", encoding="utf-8") as f:
            for i, segment in enumerate(segments_with_speaker, 1):
                # è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼
                start_time = TranscriptionFormatter._format_timestamp(segment["start"])
                end_time = TranscriptionFormatter._format_timestamp(segment["end"])
                
                f.write(f"{i}\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"[{segment['speaker']}] {segment['text']}\n\n")
                f.flush()

    @staticmethod
    def format_json_with_speakers(segments: List[Any], diarization_result: Dict[str, Any], info: Any, output_file: str) -> None:
        """å°†è½¬å½•ç»“æœå’Œè¯´è¯äººåˆ†å‰²ç»“æœä¿å­˜ä¸ºå¸¦è¯´è¯äººæ ‡ç­¾çš„JSONæ ¼å¼"""
        result = {
            "language": info.language,
            "language_probability": info.language_probability,
            "segments": []
        }
        
        for segment in segments:
            segment_start = segment.start
            segment_end = segment.end
            
            # å¯»æ‰¾é‡å æœ€å¤šçš„è¯´è¯äºº
            best_speaker = None
            max_overlap = 0
            
            for timeline_item in diarization_result["timeline"]:
                speaker_start = timeline_item["start"]
                speaker_end = timeline_item["end"]
                
                # è®¡ç®—é‡å 
                overlap_start = max(segment_start, speaker_start)
                overlap_end = min(segment_end, speaker_end)
                overlap = max(0, overlap_end - overlap_start)
                
                if overlap > max_overlap:
                    max_overlap = overlap
                    best_speaker = timeline_item["speaker"]
            
            result["segments"].append({
                "start": segment_start,
                "end": segment_end,
                "text": segment.text.strip(),
                "speaker": best_speaker if max_overlap > 0 else "æœªçŸ¥"
            })
        
        # æ·»åŠ è¯´è¯äººä¿¡æ¯
        result["speakers"] = {}
        for speaker, segments in diarization_result["speakers"].items():
            result["speakers"][speaker] = {
                "segments": segments,
                "total_time": sum(s["end"] - s["start"] for s in segments)
            }
        
        # æ·»åŠ è¯´è¯äººç»Ÿè®¡ä¿¡æ¯
        result["speaker_stats"] = diarization_result.get("speaker_stats", {})
        result["total_duration"] = diarization_result.get("total_duration", 0)
        result["num_speakers"] = diarization_result.get("num_speakers", 0)
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

    @staticmethod
    def format_txt_with_speakers(segments: List[Any], diarization_result: Dict[str, Any], info: Any, output_file: str) -> None:
        """å°†è½¬å½•ç»“æœå’Œè¯´è¯äººåˆ†å‰²ç»“æœä¿å­˜ä¸ºå¸¦è¯´è¯äººæ ‡ç­¾çš„TXTæ ¼å¼"""
        with open(output_file, "w", encoding="utf-8") as f:
            # å†™å…¥æ–‡ä»¶å¤´ä¿¡æ¯
            f.write(f"# è½¬å½•æ–‡ä»¶: {Path(output_file).stem}\n")
            f.write(f"# è½¬å½•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# æ£€æµ‹è¯­è¨€: {info.language}, ç½®ä¿¡åº¦: {info.language_probability:.4f}\n")
            f.write(f"# æ£€æµ‹è¯´è¯äººæ•°é‡: {len(diarization_result['speakers'])}\n\n")
            
            # æ·»åŠ è¯´è¯äººç»Ÿè®¡ä¿¡æ¯
            f.write("# è¯´è¯äººç»Ÿè®¡:\n")
            for speaker, stats in diarization_result.get("speaker_stats", {}).items():
                duration_mins = stats["duration"] / 60
                f.write(f"# - {speaker}: {duration_mins:.2f}åˆ†é’Ÿ, {stats['percentage']:.2f}%\n")
            f.write("\n")
            
            # è®¡ç®—æ€»æ®µæ•°ä»¥æ˜¾ç¤ºè¿›åº¦
            segment_count = 0
            for segment in segments:
                segment_count += 1
                
                # å¯»æ‰¾é‡å æœ€å¤šçš„è¯´è¯äºº
                segment_start = segment.start
                segment_end = segment.end
                
                best_speaker = None
                max_overlap = 0
                
                for timeline_item in diarization_result["timeline"]:
                    speaker_start = timeline_item["start"]
                    speaker_end = timeline_item["end"]
                    
                    # è®¡ç®—é‡å 
                    overlap_start = max(segment_start, speaker_start)
                    overlap_end = min(segment_end, speaker_end)
                    overlap = max(0, overlap_end - overlap_start)
                    
                    if overlap > max_overlap:
                        max_overlap = overlap
                        best_speaker = timeline_item["speaker"]
                
                speaker_tag = f"[{best_speaker}]" if max_overlap > 0 else "[æœªçŸ¥]"
                line = f"{speaker_tag} [{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\n"
                f.write(line)
                f.flush()
                
                # æ¯10æ®µæ˜¾ç¤ºè¿›åº¦
                if segment_count % 10 == 0:
                    logger.info(f"å·²å¤„ç† {segment_count} ä¸ªç‰‡æ®µ...")
    
    @staticmethod
    def get_formatted_output(segments: List[Any], diarization_result: Optional[Dict[str, Any]] = None, info: Optional[Any] = None, output_format: str = "txt") -> str:
        """
        å°†è½¬å½•ç»“æœæ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…å†™å…¥ç£ç›˜
        
        Args:
            segments: è½¬å½•ç‰‡æ®µ
            diarization_result: è¯´è¯äººåˆ†å‰²ç»“æœï¼ˆå¯é€‰ï¼‰
            info: è½¬å½•ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            output_format: è¾“å‡ºæ ¼å¼ ("txt", "srt", "json")
            
        Returns:
            æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
        """
        output_buffer = io.StringIO()
        
        if diarization_result and output_format == "srt":
            # æ ¼å¼åŒ–å¸¦è¯´è¯äººçš„SRT
            segments_with_speaker = []
            
            for segment in segments:
                # æ‰¾å‡ºä¸å½“å‰ç‰‡æ®µé‡å çš„è¯´è¯äºº
                segment_start = segment.start
                segment_end = segment.end
                
                # å¯»æ‰¾é‡å æœ€å¤šçš„è¯´è¯äºº
                best_speaker = None
                max_overlap = 0
                
                for timeline_item in diarization_result["timeline"]:
                    speaker_start = timeline_item["start"]
                    speaker_end = timeline_item["end"]
                    
                    # è®¡ç®—é‡å 
                    overlap_start = max(segment_start, speaker_start)
                    overlap_end = min(segment_end, speaker_end)
                    overlap = max(0, overlap_end - overlap_start)
                    
                    if overlap > max_overlap:
                        max_overlap = overlap
                        best_speaker = timeline_item["speaker"]
                
                # æ·»åŠ è¯´è¯äººä¿¡æ¯
                segments_with_speaker.append({
                    "start": segment_start,
                    "end": segment_end,
                    "text": segment.text.strip(),
                    "speaker": best_speaker if max_overlap > 0 else "æœªçŸ¥"
                })
            
            # å†™å…¥SRTæ ¼å¼
            for i, segment in enumerate(segments_with_speaker, 1):
                # è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼
                start_time = TranscriptionFormatter._format_timestamp(segment["start"])
                end_time = TranscriptionFormatter._format_timestamp(segment["end"])
                
                output_buffer.write(f"{i}\n")
                output_buffer.write(f"{start_time} --> {end_time}\n")
                output_buffer.write(f"[{segment['speaker']}] {segment['text']}\n\n")
        
        elif diarization_result and output_format == "json":
            # æ ¼å¼åŒ–å¸¦è¯´è¯äººçš„JSON
            result = {
                "language": info.language if info else "unknown",
                "language_probability": info.language_probability if info else 0.0,
                "segments": []
            }
            
            for segment in segments:
                segment_start = segment.start
                segment_end = segment.end
                
                # å¯»æ‰¾é‡å æœ€å¤šçš„è¯´è¯äºº
                best_speaker = None
                max_overlap = 0
                
                for timeline_item in diarization_result["timeline"]:
                    speaker_start = timeline_item["start"]
                    speaker_end = timeline_item["end"]
                    
                    # è®¡ç®—é‡å 
                    overlap_start = max(segment_start, speaker_start)
                    overlap_end = min(segment_end, speaker_end)
                    overlap = max(0, overlap_end - overlap_start)
                    
                    if overlap > max_overlap:
                        max_overlap = overlap
                        best_speaker = timeline_item["speaker"]
                
                result["segments"].append({
                    "start": segment_start,
                    "end": segment_end,
                    "text": segment.text.strip(),
                    "speaker": best_speaker if max_overlap > 0 else "æœªçŸ¥"
                })
            
            # æ·»åŠ è¯´è¯äººä¿¡æ¯
            result["speakers"] = {}
            for speaker, segments in diarization_result["speakers"].items():
                result["speakers"][speaker] = {
                    "segments": segments,
                    "total_time": sum(s["end"] - s["start"] for s in segments)
                }
            
            # æ·»åŠ è¯´è¯äººç»Ÿè®¡ä¿¡æ¯
            result["speaker_stats"] = diarization_result.get("speaker_stats", {})
            result["total_duration"] = diarization_result.get("total_duration", 0)
            result["num_speakers"] = diarization_result.get("num_speakers", 0)
            
            # å†™å…¥JSONæ ¼å¼
            json.dump(result, output_buffer, ensure_ascii=False, indent=2)
        
        elif diarization_result and output_format == "txt":
            # æ ¼å¼åŒ–å¸¦è¯´è¯äººçš„TXT
            # å†™å…¥æ–‡ä»¶å¤´ä¿¡æ¯
            output_buffer.write(f"# è½¬å½•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            if info:
                output_buffer.write(f"# æ£€æµ‹è¯­è¨€: {info.language}, ç½®ä¿¡åº¦: {info.language_probability:.4f}\n")
            output_buffer.write(f"# æ£€æµ‹è¯´è¯äººæ•°é‡: {len(diarization_result['speakers'])}\n\n")
            
            # æ·»åŠ è¯´è¯äººç»Ÿè®¡ä¿¡æ¯
            output_buffer.write("# è¯´è¯äººç»Ÿè®¡:\n")
            for speaker, stats in diarization_result.get("speaker_stats", {}).items():
                duration_mins = stats["duration"] / 60
                output_buffer.write(f"# - {speaker}: {duration_mins:.2f}åˆ†é’Ÿ, {stats['percentage']:.2f}%\n")
            output_buffer.write("\n")
            
            # è®¡ç®—æ€»æ®µæ•°ä»¥æ˜¾ç¤ºè¿›åº¦
            for segment in segments:
                # å¯»æ‰¾é‡å æœ€å¤šçš„è¯´è¯äºº
                segment_start = segment.start
                segment_end = segment.end
                
                best_speaker = None
                max_overlap = 0
                
                for timeline_item in diarization_result["timeline"]:
                    speaker_start = timeline_item["start"]
                    speaker_end = timeline_item["end"]
                    
                    # è®¡ç®—é‡å 
                    overlap_start = max(segment_start, speaker_start)
                    overlap_end = min(segment_end, speaker_end)
                    overlap = max(0, overlap_end - overlap_start)
                    
                    if overlap > max_overlap:
                        max_overlap = overlap
                        best_speaker = timeline_item["speaker"]
                
                speaker_tag = f"[{best_speaker}]" if max_overlap > 0 else "[æœªçŸ¥]"
                line = f"{speaker_tag} [{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\n"
                output_buffer.write(line)
        
        elif output_format == "srt":
            # æ™®é€šSRTæ ¼å¼
            for i, segment in enumerate(segments, 1):
                # è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼ (HH:MM:SS,mmm)
                start_time = TranscriptionFormatter._format_timestamp(segment.start)
                end_time = TranscriptionFormatter._format_timestamp(segment.end)
                
                output_buffer.write(f"{i}\n")
                output_buffer.write(f"{start_time} --> {end_time}\n")
                output_buffer.write(f"{segment.text.strip()}\n\n")
        
        elif output_format == "json":
            # æ™®é€šJSONæ ¼å¼
            result = {
                "language": info.language if info else "unknown",
                "language_probability": info.language_probability if info else 0.0,
                "segments": []
            }
            
            for segment in segments:
                result["segments"].append({
                    "start": segment.start,
                    "end": segment.end,
                    "text": segment.text.strip()
                })
            
            # å†™å…¥JSONæ ¼å¼
            json.dump(result, output_buffer, ensure_ascii=False, indent=2)
        
        else:  # é»˜è®¤txtæ ¼å¼
            # æ™®é€šTXTæ ¼å¼
            if info:
                output_buffer.write(f"# è½¬å½•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                output_buffer.write(f"# æ£€æµ‹è¯­è¨€: {info.language}, ç½®ä¿¡åº¦: {info.language_probability:.4f}\n\n")
            
            for segment in segments:
                line = f"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\n"
                output_buffer.write(line)
        
        return output_buffer.getvalue()


class AudioProcessor:
    """éŸ³é¢‘å¤„ç†å·¥å…·ç±»"""
    
    # å®šä¹‰æ”¯æŒçš„æ ¼å¼åˆ—è¡¨
    COMPATIBLE_FORMATS = [".wav"]  # é»˜è®¤å…¼å®¹çš„æ ¼å¼
    SUPPORTED_FORMATS = [".wav", ".flac", ".mp3", ".m4a", ".ogg", ".opus"]  # æ”¯æŒè½¬æ¢çš„æ ¼å¼
    
    @staticmethod
    def check_ffmpeg_available() -> bool:
        """
        æ£€æŸ¥ffmpegæ˜¯å¦å¯ç”¨
        
        Returns:
            æ˜¯å¦å¯ç”¨
        """
        try:
            import subprocess
            # å°è¯•è¿è¡Œffmpegå‘½ä»¤ - ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼é¿å…ç¼–ç é—®é¢˜
            result = subprocess.run(
                ["ffmpeg", "-version"], 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE,
                text=False,  # ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼
                check=False
            )
            if result.returncode == 0:
                # å®‰å…¨è§£ç stdout
                stdout_text = result.stdout.decode('utf-8', errors='replace') if result.stdout else ""
                ffmpeg_version = stdout_text.split('\n')[0] if stdout_text else "æœªçŸ¥ç‰ˆæœ¬"
                logger.debug(f"æ£€æµ‹åˆ°FFmpeg: {ffmpeg_version}")
                return True
            else:
                logger.debug("FFmpegå‘½ä»¤æ‰§è¡Œå¤±è´¥")
                return False
        except Exception as e:
            logger.debug(f"FFmpegæ£€æµ‹å¤±è´¥: {e}")
            return False
    
    @staticmethod
    def convert_audio_with_ffmpeg(audio_file: str, output_path: str, output_format: str = "wav") -> bool:
        """
        ä½¿ç”¨FFmpegå°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼ï¼Œä¿ç•™åŸå§‹éŸ³é¢‘è´¨é‡ç‰¹æ€§
        
        Args:
            audio_file: åŸå§‹éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            output_format: è¾“å‡ºæ ¼å¼(wav, flacç­‰)
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            import subprocess
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # æ ¹æ®è¾“å‡ºæ ¼å¼è®¾ç½®ç¼–ç å‚æ•°ï¼Œä¿ç•™è¾“å…¥éŸ³é¢‘ç‰¹æ€§
            codec_params = []
            if output_format.lower() == "wav":
                # PCMæ ¼å¼ï¼Œä¸åšç‰¹æ®ŠæŒ‡å®šè®©FFmpegè‡ªåŠ¨åŒ¹é…åŸå§‹é‡‡æ ·ç‡å’Œé€šé“æ•°
                codec_params = ["-acodec", "pcm_s24le"]  # ä»…æŒ‡å®šä½æ·±åº¦ä¸º24ä½
            elif output_format.lower() == "flac":
                # FLACæ— æŸå‹ç¼©ï¼Œä¿ç•™åŸå§‹éŸ³é¢‘ç‰¹æ€§
                codec_params = [
                    "-acodec", "flac",
                    "-compression_level", "12"  # æœ€é«˜å‹ç¼©çº§åˆ«ï¼Œä¸å½±å“è´¨é‡
                ]
            else:
                # å…¶ä»–æ ¼å¼å°½é‡ä½¿ç”¨å¤åˆ¶æ¨¡å¼
                codec_params = ["-acodec", "copy"]
                logger.info(f"ä½¿ç”¨å¤åˆ¶æ¨¡å¼è½¬æ¢ä¸º{output_format}ï¼Œä¿æŒåŸå§‹éŸ³é¢‘æµ")
            
            # æ„å»ºffmpegå‘½ä»¤
            cmd = ["ffmpeg", "-i", audio_file, "-y"] + codec_params + [output_path]
            
            logger.debug(f"è¿è¡ŒFFmpegå‘½ä»¤: {' '.join(cmd)}")
            
            # æ‰§è¡Œå‘½ä»¤ - ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼é¿å…ç¼–ç é—®é¢˜
            process = subprocess.run(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=False,  # ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼
                check=False
            )
            
            if process.returncode == 0:
                logger.info(f"FFmpegè½¬æ¢æˆåŠŸ: {output_path}")
                return True
            else:
                # å®‰å…¨è§£ç stderrï¼Œä½¿ç”¨replaceç­–ç•¥å¤„ç†æ— æ³•è§£ç çš„å­—ç¬¦
                stderr_text = process.stderr.decode('utf-8', errors='replace') if process.stderr else ""
                logger.warning(f"FFmpegè½¬æ¢å¤±è´¥: {stderr_text}")
                
                # å°è¯•ä½¿ç”¨æœ€ç®€å•çš„è½¬æ¢å‚æ•°
                logger.info("å°è¯•ä½¿ç”¨åŸºæœ¬è®¾ç½®è¿›è¡Œè½¬æ¢...")
                basic_cmd = ["ffmpeg", "-i", audio_file, "-y", output_path]
                
                basic_process = subprocess.run(
                    basic_cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=False,
                    check=False
                )
                
                if basic_process.returncode == 0:
                    logger.info(f"ä½¿ç”¨åŸºæœ¬è®¾ç½®è½¬æ¢æˆåŠŸ: {output_path}")
                    return True
                else:
                    basic_stderr_text = basic_process.stderr.decode('utf-8', errors='replace') if basic_process.stderr else ""
                    logger.error(f"åŸºæœ¬è®¾ç½®è½¬æ¢ä¹Ÿå¤±è´¥: {basic_stderr_text}")
                    return False
                
        except Exception as e:
            logger.error(f"FFmpegè½¬æ¢é”™è¯¯: {e}")
            return False
    
    @staticmethod
    def convert_to_format(
        audio_file: str, 
        target_format: str = "wav", 
        output_dir: Optional[str] = None, 
        auto_cleanup: bool = True
    ) -> str:
        """
        å°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼ï¼Œä½¿ç”¨FFmpegè¿›è¡Œæ— æŸè½¬æ¢
        
        Args:
            audio_file: åŸå§‹éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            target_format: ç›®æ ‡æ ¼å¼('wav', 'flac'ç­‰)ï¼Œä¸åŒ…å«ç‚¹
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨åŸæ–‡ä»¶ç›®å½•ï¼‰
            auto_cleanup: æ˜¯å¦è‡ªåŠ¨å°†è½¬æ¢åçš„æ–‡ä»¶æ·»åŠ åˆ°æ¸…ç†åˆ—è¡¨
            
        Returns:
            è½¬æ¢åçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        audio_path = Path(audio_file)
        current_format = audio_path.suffix.lower()
        target_format = target_format.lower()
        target_ext = f".{target_format}" if not target_format.startswith(".") else target_format
        
        # å¦‚æœå·²ç»æ˜¯ç›®æ ‡æ ¼å¼ï¼Œåˆ™ç›´æ¥è¿”å›
        if current_format == target_ext:
            return str(audio_path)
        
        logger.info(f"è½¬æ¢éŸ³é¢‘æ ¼å¼: {audio_file} -> {target_format.upper()}")
        
        # ç¡®å®šè¾“å‡ºè·¯å¾„
        if output_dir:
            output_path = Path(output_dir) / f"{audio_path.stem}{target_ext}"
            os.makedirs(output_dir, exist_ok=True)
        else:
            # å¦‚æœæœªæŒ‡å®šè¾“å‡ºç›®å½•ï¼Œä½¿ç”¨ä¸åŸå§‹æ–‡ä»¶ç›¸åŒçš„ç›®å½•
            output_path = audio_path.parent / f"{audio_path.stem}{target_ext}"
        
        # æ£€æŸ¥FFmpegæ˜¯å¦å¯ç”¨
        ffmpeg_available = AudioProcessor.check_ffmpeg_available()
        
        if not ffmpeg_available:
            logger.error("FFmpegä¸å¯ç”¨ï¼Œæ— æ³•è½¬æ¢éŸ³é¢‘æ ¼å¼")
            logger.error("è¯·å®‰è£…FFmpegå¹¶ç¡®ä¿å…¶åœ¨ç³»ç»ŸPATHä¸­ï¼Œæˆ–ä½¿ç”¨--ffmpeg-pathå‚æ•°æŒ‡å®šå…¶è·¯å¾„")
            return audio_file
        
        # ä½¿ç”¨FFmpegè¿›è¡Œè½¬æ¢
        logger.info(f"ä½¿ç”¨FFmpegè¿›è¡Œæ— æŸéŸ³é¢‘æ ¼å¼è½¬æ¢ä¸º{target_format}")
        conversion_success = AudioProcessor.convert_audio_with_ffmpeg(
            audio_file, 
            str(output_path),
            target_format
        )
        
        if conversion_success:
            logger.info(f"éŸ³é¢‘æ ¼å¼è½¬æ¢å®Œæˆ: {output_path}")
            
            # å°†ç”Ÿæˆçš„æ–‡ä»¶æ·»åŠ åˆ°æ¸…ç†åˆ—è¡¨
            if auto_cleanup:
                global TEMP_FILES
                TEMP_FILES.append(str(output_path))
                logger.debug(f"æ·»åŠ ä¸´æ—¶{target_format}æ–‡ä»¶åˆ°æ¸…ç†åˆ—è¡¨: {output_path}")
            
            return str(output_path)
        else:
            logger.error(f"æ— æ³•è½¬æ¢åˆ°{target_format}æ ¼å¼ï¼Œè¿”å›åŸå§‹æ–‡ä»¶")
            return audio_file
    
    @staticmethod
    def is_format_compatible(audio_file: str, for_diarization: bool = False) -> bool:
        """
        æ£€æŸ¥éŸ³é¢‘æ ¼å¼æ˜¯å¦ä¸å¤„ç†å…¼å®¹
        
        Args:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            for_diarization: æ˜¯å¦ç”¨äºè¯´è¯äººåˆ†å‰²ï¼ˆæ›´ä¸¥æ ¼çš„å…¼å®¹æ€§è¦æ±‚ï¼‰
            
        Returns:
            æ˜¯å¦å…¼å®¹
        """
        audio_path = Path(audio_file)
        audio_ext = audio_path.suffix.lower()
        
        # è¯´è¯äººåˆ†å‰²å¯¹æ ¼å¼è¦æ±‚æ›´ä¸¥æ ¼
        if for_diarization:
            return audio_ext in AudioProcessor.COMPATIBLE_FORMATS
        
        # ä¸€èˆ¬è½¬å½•æ”¯æŒæ›´å¤šæ ¼å¼
        return audio_ext in AudioProcessor.SUPPORTED_FORMATS
    
    @staticmethod
    def convert_to_wav(audio_file: str, output_dir: Optional[str] = None, auto_cleanup: bool = True) -> str:
        """
        å°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºWAVæ ¼å¼ï¼ˆä¸ºäº†å‘åå…¼å®¹ï¼‰
        
        Args:
            audio_file: åŸå§‹éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            auto_cleanup: æ˜¯å¦è‡ªåŠ¨å°†è½¬æ¢åçš„æ–‡ä»¶æ·»åŠ åˆ°æ¸…ç†åˆ—è¡¨
            
        Returns:
            è½¬æ¢åçš„WAVæ–‡ä»¶è·¯å¾„
        """
        return AudioProcessor.convert_to_format(audio_file, "wav", output_dir, auto_cleanup)
    
    @staticmethod
    def convert_to_wav_with_ffmpeg(audio_file: str, output_path: str) -> bool:
        """
        ä½¿ç”¨FFmpegå°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºWAVæ ¼å¼ï¼ˆä¸ºäº†å‘åå…¼å®¹ï¼‰
        
        Args:
            audio_file: åŸå§‹éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        return AudioProcessor.convert_audio_with_ffmpeg(audio_file, output_path, "wav")
    
    @staticmethod
    def cleanup_temp_files():
        """æ¸…ç†æ‰€æœ‰ä¸´æ—¶ç”Ÿæˆçš„æ–‡ä»¶"""
        global TEMP_FILES
        success_count = 0
        
        for temp_file in TEMP_FILES:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
                    logger.debug(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {temp_file}")
                    success_count += 1
            except Exception as e:
                logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {temp_file}, é”™è¯¯: {e}")
        
        logger.info(f"ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ: æˆåŠŸåˆ é™¤ {success_count}/{len(TEMP_FILES)} ä¸ªæ–‡ä»¶")
        TEMP_FILES.clear()


class WhisperTranscriber:
    """è¯­éŸ³è½¬å½•å™¨æ ¸å¿ƒç±»"""
    
    def __init__(self, model_path: str, device: str = "auto", compute_type: str = "auto"):
        """
        åˆå§‹åŒ–è½¬å½•å™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ç±»å‹ ("cuda", "cpu", "auto")
            compute_type: è®¡ç®—ç²¾åº¦ç±»å‹ ("float16", "float32", "int8", "auto")
        """
        self.model_path = model_path
        
        # ç”¨æˆ·æ˜ç¡®æŒ‡å®šè®¾å¤‡æ—¶çš„å¤„ç†
        if device == "cuda":
            logger.info("ç”¨æˆ·æ˜ç¡®æŒ‡å®šä½¿ç”¨CUDAè®¾å¤‡")
            self.device = "cuda"
            self.compute_type = compute_type if compute_type != "auto" else "float16"
        elif device == "cpu":
            logger.info("ç”¨æˆ·æ˜ç¡®æŒ‡å®šä½¿ç”¨CPUè®¾å¤‡")
            self.device = "cpu"
            self.compute_type = compute_type if compute_type != "auto" else "float32"
        else:  # autoæ¨¡å¼
            # è·å–è‡ªåŠ¨æ£€æµ‹ç»“æœ
            gpu_status = SafeGPUDetector.check_cuda_available()
            self.device = gpu_status["device"]
            
            if compute_type != "auto":
                self.compute_type = compute_type
            else:
                self.compute_type = gpu_status["compute_type"]
            
            # æ˜¾ç¤ºè¯¦ç»†çš„æ£€æµ‹ç»“æœ
            if gpu_status["fallback_reason"]:
                logger.info(f"GPUæ£€æµ‹å›é€€åˆ°CPU: {gpu_status['fallback_reason']}")
            
            logger.info(f"è‡ªåŠ¨é€‰æ‹©è®¾å¤‡: {self.device}, è®¡ç®—ç±»å‹: {self.compute_type}")
        
        # å»¶è¿ŸåŠ è½½æ¨¡å‹ï¼Œé¿å…ä¸å¿…è¦çš„èµ„æºæ¶ˆè€—
        self._model = None
        
        # å†…å­˜ç¼“å­˜
        self._audio_cache = {}
    
    @property
    def model(self):
        """å»¶è¿ŸåŠ è½½æ¨¡å‹"""
        if self._model is None:
            try:
                from faster_whisper import WhisperModel
                
                logger.info(f"åŠ è½½æ¨¡å‹: {self.model_path}")
                logger.info(f"è®¾å¤‡: {self.device}, è®¡ç®—ç±»å‹: {self.compute_type}")
                
                # è®°å½•åŠ è½½å¼€å§‹æ—¶é—´
                load_start = time.time()
                
                self._model = WhisperModel(
                    model_size_or_path=self.model_path,
                    device=self.device,
                    compute_type=self.compute_type,
                    local_files_only=True,
                    download_root=None
                )
                
                load_time = time.time() - load_start
                logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}s")
                
            except ImportError as e:
                logger.error(f"å¯¼å…¥faster_whisperå¤±è´¥: {e}")
                logger.error("è¯·ç¡®ä¿å·²å®‰è£… faster_whisper: pip install faster-whisper")
                raise
            except Exception as e:
                logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                raise
        
        return self._model
    
    def _preload_audio(self, audio_file: str) -> None:
        """é¢„åŠ è½½éŸ³é¢‘åˆ°å†…å­˜ç¼“å­˜"""
        if audio_file not in self._audio_cache:
            try:
                import soundfile as sf
                logger.debug(f"é¢„åŠ è½½éŸ³é¢‘åˆ°å†…å­˜: {audio_file}")
                # ä½¿ç”¨soundfileåŠ è½½éŸ³é¢‘æ•°æ®
                data, sample_rate = sf.read(audio_file)
                self._audio_cache[audio_file] = {
                    "data": data,
                    "sample_rate": sample_rate,
                    "temp_file": None
                }
                logger.debug(f"éŸ³é¢‘é¢„åŠ è½½å®Œæˆï¼Œæ ·æœ¬ç‡: {sample_rate}Hz, æ•°æ®å¤§å°: {data.shape}")
            except Exception as e:
                logger.warning(f"éŸ³é¢‘é¢„åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨ç›´æ¥æ–‡ä»¶åŠ è½½: {e}")
    
    def _clear_cache(self, audio_file: Optional[str] = None):
        """æ¸…ç†å†…å­˜ç¼“å­˜"""
        if audio_file:
            if audio_file in self._audio_cache:
                # å¦‚æœæœ‰ä¸´æ—¶æ–‡ä»¶ï¼Œæ¸…ç†å®ƒ
                if self._audio_cache[audio_file]["temp_file"] and os.path.exists(self._audio_cache[audio_file]["temp_file"]):
                    try:
                        os.remove(self._audio_cache[audio_file]["temp_file"])
                    except Exception as e:
                        logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
                
                del self._audio_cache[audio_file]
                logger.debug(f"å·²æ¸…ç†éŸ³é¢‘ç¼“å­˜: {audio_file}")
        else:
            # æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
            for cache_info in self._audio_cache.values():
                if cache_info["temp_file"] and os.path.exists(cache_info["temp_file"]):
                    try:
                        os.remove(cache_info["temp_file"])
                    except Exception as e:
                        logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
            
            self._audio_cache.clear()
            logger.debug("å·²æ¸…ç†æ‰€æœ‰éŸ³é¢‘ç¼“å­˜")
    
    @timer
    def transcribe(
        self,
        audio_file: str,
        beam_size: int = 5,
        language: str = "zh",
        task: str = "transcribe",
        vad_filter: bool = True,
        min_silence_ms: int = 1000,
        preload_audio: bool = True
    ) -> Tuple[List[Any], Any]:
        """
        è½¬å½•éŸ³é¢‘æ–‡ä»¶
        
        Args:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            beam_size: Beam searchå¤§å°
            language: è¯­è¨€ä»£ç 
            task: ä»»åŠ¡ç±»å‹ ("transcribe", "translate")
            vad_filter: æ˜¯å¦ä½¿ç”¨è¯­éŸ³æ´»åŠ¨æ£€æµ‹è¿‡æ»¤
            min_silence_ms: æœ€å°é™éŸ³æ—¶é•¿(æ¯«ç§’)
            preload_audio: æ˜¯å¦é¢„åŠ è½½éŸ³é¢‘åˆ°å†…å­˜
            
        Returns:
            (segmentsåˆ—è¡¨, è½¬å½•ä¿¡æ¯)
        """
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        audio_path = Path(audio_file)
        if not audio_path.exists():
            raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")
        
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        audio_ext = audio_path.suffix.lower()
        supported_formats = [".wav", ".mp3", ".m4a", ".flac", ".ogg", ".opus"]
        if audio_ext not in supported_formats:
            logger.warning(f"éŸ³é¢‘æ ¼å¼ {audio_ext} å¯èƒ½ä¸å—æ”¯æŒï¼Œæ”¯æŒçš„æ ¼å¼: {', '.join(supported_formats)}")
        
        logger.info(f"å¼€å§‹è½¬å½•: {audio_file}")
        
        # é¢„åŠ è½½éŸ³é¢‘åˆ°å†…å­˜
        audio_input = audio_file
        if preload_audio:
            self._preload_audio(audio_file)
            if audio_file in self._audio_cache and not self._audio_cache[audio_file]["temp_file"]:
                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                try:
                    import soundfile as sf
                    
                    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=audio_ext)
                    temp_filename = temp_file.name
                    temp_file.close()
                    
                    # å†™å…¥ä¸´æ—¶æ–‡ä»¶
                    sf.write(temp_filename, self._audio_cache[audio_file]["data"], 
                             self._audio_cache[audio_file]["sample_rate"])
                    
                    # æ›´æ–°ç¼“å­˜
                    self._audio_cache[audio_file]["temp_file"] = temp_filename
                    audio_input = temp_filename
                    
                    logger.debug(f"å·²åˆ›å»ºéŸ³é¢‘ä¸´æ—¶æ–‡ä»¶: {temp_filename}")
                except Exception as e:
                    logger.warning(f"åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸå§‹æ–‡ä»¶: {e}")
        
        try:
            # è½¬å½•é…ç½®
            vad_parameters = dict(min_silence_duration_ms=min_silence_ms) if vad_filter else None
            
            segments_iter, info = self.model.transcribe(
                audio_input,
                beam_size=beam_size,
                language=language,
                task=task,
                vad_filter=vad_filter,
                vad_parameters=vad_parameters,
            )
            
            # è½¬æ¢è¿­ä»£å™¨ä¸ºåˆ—è¡¨ï¼Œé¿å…è¿­ä»£å™¨åªèƒ½ä½¿ç”¨ä¸€æ¬¡çš„é—®é¢˜
            segments = list(segments_iter)
            
            return segments, info
        
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if preload_audio and audio_file in self._audio_cache and audio_input != audio_file:
                try:
                    # ä¸´æ—¶æ–‡ä»¶ä¼šåœ¨_clear_cacheä¸­åˆ é™¤
                    pass
                except Exception as e:
                    logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
    
    @timer
    def transcribe_and_save(
        self,
        audio_file: str,
        output_file: str = "transcription.txt",
        output_format: str = "txt",
        in_memory: bool = False,
        **kwargs
    ) -> str:
        """
        è½¬å½•éŸ³é¢‘å¹¶ä¿å­˜ç»“æœ
        
        Args:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            output_format: è¾“å‡ºæ ¼å¼ ("txt", "srt", "json")
            in_memory: æ˜¯å¦åœ¨å†…å­˜ä¸­å¤„ç†ï¼ˆä¸å†™å…¥ç£ç›˜ï¼‰
            **kwargs: å…¶ä»–è½¬å½•å‚æ•°
            
        Returns:
            è¾“å‡ºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„æˆ–å†…å­˜ä¸­çš„ç»“æœå­—ç¬¦ä¸²
        """
        # è·å–è½¬å½•ç»“æœ
        segments, info = self.transcribe(audio_file, **kwargs)
        
        # å¦‚æœin_memoryä¸ºTrueï¼Œåˆ™ä¸å†™å…¥ç£ç›˜ï¼Œç›´æ¥è¿”å›æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
        if in_memory:
            return TranscriptionFormatter.get_formatted_output(
                segments, 
                None, 
                info, 
                output_format
            )
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # æ ¹æ®è¾“å‡ºæ ¼å¼ä¿å­˜ç»“æœ
        if output_format == "srt":
            TranscriptionFormatter.format_srt(segments, output_file)
        elif output_format == "json":
            TranscriptionFormatter.format_json(segments, info, output_file)
        else:  # é»˜è®¤txtæ ¼å¼
            with open(output_file, "w", encoding="utf-8") as f:
                # å†™å…¥æ–‡ä»¶å¤´ä¿¡æ¯
                f.write(f"# è½¬å½•æ–‡ä»¶: {Path(audio_file).name}\n")
                f.write(f"# è½¬å½•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"# æ£€æµ‹è¯­è¨€: {info.language}, ç½®ä¿¡åº¦: {info.language_probability:.4f}\n\n")
                
                # è®¡ç®—æ€»æ®µæ•°ä»¥æ˜¾ç¤ºè¿›åº¦
                segment_count = 0
                for segment in segments:
                    segment_count += 1
                    line = f"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\n"
                    f.write(line)
                    f.flush()
                    
                    # æ¯10æ®µæ˜¾ç¤ºè¿›åº¦
                    if segment_count % 10 == 0:
                        logger.info(f"å·²å¤„ç† {segment_count} ä¸ªç‰‡æ®µ...")
        
        # æ¸…ç†éŸ³é¢‘ç¼“å­˜
        self._clear_cache(audio_file)
        
        logger.info(f"è½¬å½•å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {output_path.absolute()}")
        return str(output_path.absolute())
    
    @timer
    def transcribe_with_diarization(
        self,
        audio_file: str,
        diarization: 'SpeakerDiarization',
        num_speakers: Optional[int] = None,
        min_speakers: Optional[int] = None,
        max_speakers: Optional[int] = None,
        audio_format: str = "auto",
        **kwargs
    ) -> Tuple[List[Any], Any, Dict[str, Any]]:
        """
        è½¬å½•éŸ³é¢‘å¹¶è¿›è¡Œè¯´è¯äººåˆ†å‰²
        
        Args:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            diarization: è¯´è¯äººåˆ†å‰²å¤„ç†å™¨
            num_speakers: æŒ‡å®šè¯´è¯äººæ•°é‡ï¼ˆå¦‚æœå·²çŸ¥ï¼‰
            min_speakers: æœ€å°è¯´è¯äººæ•°é‡
            max_speakers: æœ€å¤§è¯´è¯äººæ•°é‡
            audio_format: éŸ³é¢‘æ ¼å¼å¤„ç†æ–¹å¼ ("auto", "wav", "flac"ç­‰)
            **kwargs: å…¶ä»–è½¬å½•å‚æ•°
            
        Returns:
            (segmentsåˆ—è¡¨, è½¬å½•ä¿¡æ¯, è¯´è¯äººåˆ†å‰²ç»“æœ)
        """
        # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ ¼å¼
        audio_path = Path(audio_file)
        
        # å¤„ç†éŸ³é¢‘æ ¼å¼
        processed_audio = audio_file
        if audio_format == "auto":
            # è‡ªåŠ¨æ¨¡å¼ï¼šå¦‚æœæ ¼å¼ä¸å…¼å®¹è¯´è¯äººåˆ†å‰²ï¼Œè½¬æ¢ä¸ºwav
            if not AudioProcessor.is_format_compatible(audio_file, for_diarization=True):
                logger.info(f"æ£€æµ‹åˆ°{audio_path.suffix}æ ¼å¼éŸ³é¢‘æ–‡ä»¶ï¼ŒPyannoteå¯èƒ½æ— æ³•ç›´æ¥å¤„ç†")
                # ä½¿ç”¨ä¸è¾“å‡ºæ–‡ä»¶ç›¸åŒçš„ç›®å½•ä¿å­˜è½¬æ¢åçš„wavæ–‡ä»¶
                output_dir = audio_path.parent
                processed_audio = AudioProcessor.convert_to_wav(audio_file, output_dir=output_dir)
                logger.info(f"å·²å°†{audio_path.suffix}æ ¼å¼è½¬æ¢ä¸ºwav: {processed_audio}")
        elif audio_format.lower() != "none":
            # æŒ‡å®šæ ¼å¼æ¨¡å¼ï¼šå¼ºåˆ¶è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼
            logger.info(f"æ ¹æ®ç”¨æˆ·è®¾ç½®ï¼Œå°†éŸ³é¢‘è½¬æ¢ä¸º{audio_format}æ ¼å¼")
            output_dir = audio_path.parent
            processed_audio = AudioProcessor.convert_to_format(audio_file, audio_format, output_dir=output_dir)
            logger.info(f"éŸ³é¢‘è½¬æ¢å®Œæˆ: {processed_audio}")
        else:
            # noneæ¨¡å¼ï¼šä¸è¿›è¡Œä»»ä½•è½¬æ¢
            logger.info("ç”¨æˆ·é€‰æ‹©ä¸è¿›è¡ŒéŸ³é¢‘æ ¼å¼è½¬æ¢ï¼Œä¿æŒåŸæ ¼å¼")
        
        # é¦–å…ˆè¿›è¡Œè¯­éŸ³è½¬å½•
        segments, info = self.transcribe(audio_file, **kwargs)
        
        # ç„¶åæ‰§è¡Œè¯´è¯äººåˆ†å‰²
        diarization_result = diarization.process(
            processed_audio,  # ä½¿ç”¨å¤„ç†åçš„éŸ³é¢‘æ–‡ä»¶
            num_speakers=num_speakers,
            min_speakers=min_speakers,
            max_speakers=max_speakers
        )
        
        return segments, info, diarization_result

    @timer
    def transcribe_and_save_with_speakers(
        self,
        audio_file: str,
        output_file: str = "transcription_with_speakers.txt",
        output_format: str = "txt",
        in_memory: bool = False,
        diarization: Optional[SpeakerDiarization] = None,
        num_speakers: Optional[int] = None,
        min_speakers: Optional[int] = None,
        max_speakers: Optional[int] = None,
        audio_format: str = "auto",
        **kwargs
    ) -> str:
        """
        è½¬å½•éŸ³é¢‘ã€è¿›è¡Œè¯´è¯äººåˆ†å‰²å¹¶ä¿å­˜ç»“æœ
        
        Args:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            output_format: è¾“å‡ºæ ¼å¼ ("txt", "srt", "json")
            in_memory: æ˜¯å¦åœ¨å†…å­˜ä¸­å¤„ç†ï¼ˆä¸å†™å…¥ç£ç›˜ï¼‰
            diarization: è¯´è¯äººåˆ†å‰²å¤„ç†å™¨ï¼ˆå¿…é¡»æä¾›ï¼‰
            num_speakers: æŒ‡å®šè¯´è¯äººæ•°é‡ï¼ˆå¦‚æœå·²çŸ¥ï¼‰
            min_speakers: æœ€å°è¯´è¯äººæ•°é‡
            max_speakers: æœ€å¤§è¯´è¯äººæ•°é‡
            audio_format: éŸ³é¢‘æ ¼å¼å¤„ç†æ–¹å¼ ("auto", "wav", "flac"ç­‰)
            **kwargs: å…¶ä»–è½¬å½•å‚æ•°
            
        Returns:
            è¾“å‡ºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„æˆ–å†…å­˜ä¸­çš„ç»“æœå­—ç¬¦ä¸²
        """
        # ç¡®ä¿æä¾›äº†è¯´è¯äººåˆ†å‰²å¤„ç†å™¨
        if diarization is None:
            raise ValueError("å¿…é¡»æä¾›è¯´è¯äººåˆ†å‰²å¤„ç†å™¨(diarization)å‚æ•°")
        
        try:
            # è·å–è½¬å½•å’Œè¯´è¯äººåˆ†å‰²ç»“æœ
            segments, info, diarization_result = self.transcribe_with_diarization(
                audio_file,
                diarization=diarization,
                num_speakers=num_speakers,
                min_speakers=min_speakers,
                max_speakers=max_speakers,
                audio_format=audio_format,
                **kwargs
            )
            
            # å¦‚æœin_memoryä¸ºTrueï¼Œåˆ™ä¸å†™å…¥ç£ç›˜ï¼Œç›´æ¥è¿”å›æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
            if in_memory:
                return TranscriptionFormatter.get_formatted_output(
                    segments, 
                    diarization_result, 
                    info, 
                    output_format
                )
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # æ ¹æ®è¾“å‡ºæ ¼å¼ä¿å­˜ç»“æœ
            if output_format == "srt":
                TranscriptionFormatter.format_srt_with_speakers(segments, diarization_result, output_file)
            elif output_format == "json":
                TranscriptionFormatter.format_json_with_speakers(segments, diarization_result, info, output_file)
            else:  # é»˜è®¤txtæ ¼å¼
                TranscriptionFormatter.format_txt_with_speakers(segments, diarization_result, info, output_file)
            
            logger.info(f"è½¬å½•ä¸è¯´è¯äººåˆ†å‰²å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {output_path.absolute()}")
            return str(output_path.absolute())
            
        finally:
            # æ¸…ç†éŸ³é¢‘ç¼“å­˜
            self._clear_cache(audio_file)
    
    def close(self):
        """é‡Šæ”¾æ¨¡å‹èµ„æº"""
        if self._model is not None:
            del self._model
            self._model = None
        self._clear_cache()


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="è¯­éŸ³è½¬æ–‡å­—å·¥å…· - åŸºäºFasterWhisperä¸Pyannote",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # å¿…é€‰å‚æ•°
    parser.add_argument("--audio", "-a", required=True, help="éŸ³é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", "-m", required=True, help="Whisperæ¨¡å‹è·¯å¾„")
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument("--output", "-o", default="transcription.txt", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--format", "-f", choices=["txt", "srt", "json"], default="txt",
                      help="è¾“å‡ºæ ¼å¼")
    
    # è½¬å½•å‚æ•°
    parser.add_argument("--beam-size", "-b", type=int, default=5, help="Beam searchå¤§å°")
    parser.add_argument("--language", "-l", default="zh", help="è¯­è¨€ä»£ç ")
    parser.add_argument("--task", "-t", choices=["transcribe", "translate"], default="transcribe",
                      help="ä»»åŠ¡ç±»å‹")
    parser.add_argument("--no-vad", action="store_false", dest="vad_filter",
                      help="ç¦ç”¨è¯­éŸ³æ´»åŠ¨æ£€æµ‹")
    parser.add_argument("--silence", "-s", type=int, default=1000,
                      help="æœ€å°é™éŸ³æ—¶é•¿(æ¯«ç§’)")
    
    # è®¾å¤‡å‚æ•°
    parser.add_argument("--device", "-d", choices=["cuda", "cpu", "auto"], default="auto",
                      help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--compute-type", "-c", 
                      choices=["float16", "float32", "int8", "auto"], default="auto",
                      help="è®¡ç®—ç²¾åº¦ç±»å‹")
    
    # æ‰¹å¤„ç†å‚æ•°
    parser.add_argument("--batch", action="store_true", help="æ‰¹å¤„ç†æ¨¡å¼")
    parser.add_argument("--output-dir", default="transcriptions", 
                      help="æ‰¹å¤„ç†æ¨¡å¼ä¸‹çš„è¾“å‡ºç›®å½•")
    parser.add_argument("--in-memory", action="store_true", 
                      help="åœ¨å†…å­˜ä¸­å¤„ç†è€Œä¸æ˜¯å†™å…¥ä¸´æ—¶æ–‡ä»¶")
    
    # è¯´è¯äººåˆ†å‰²å‚æ•°
    parser.add_argument("--speakers", action="store_true", help="å¯ç”¨è¯´è¯äººåˆ†å‰²")
    parser.add_argument("--speaker-model", required=False, help="è¯´è¯äººåˆ†å‰²æœ¬åœ°æ¨¡å‹è·¯å¾„æˆ–Hugging Faceæ¨¡å‹ID")
    parser.add_argument("--num-speakers", type=int, help="æŒ‡å®šè¯´è¯äººæ•°é‡ï¼ˆå¦‚æœå·²çŸ¥ï¼‰")
    parser.add_argument("--min-speakers", type=int, default=1, help="æœ€å°è¯´è¯äººæ•°é‡")
    parser.add_argument("--max-speakers", type=int, default=5, help="æœ€å¤§è¯´è¯äººæ•°é‡")
    
    # éŸ³é¢‘æ ¼å¼å‚æ•°
    parser.add_argument("--audio-format", choices=["auto", "wav", "flac", "none"], default="auto",
                      help="éŸ³é¢‘æ ¼å¼å¤„ç†æ–¹å¼: auto(è‡ªåŠ¨æ£€æµ‹å¹¶è½¬æ¢ä¸å…¼å®¹æ ¼å¼), wav(å¼ºåˆ¶è½¬æ¢ä¸ºWAV), "
                           "flac(å¼ºåˆ¶è½¬æ¢ä¸ºFLAC), none(ä¸è¿›è¡Œè½¬æ¢)")
    
    # ç¯å¢ƒä¸è·¯å¾„å‚æ•°
    env_group = parser.add_argument_group('ç¯å¢ƒä¸è·¯å¾„é…ç½®')
    env_group.add_argument("--cuda-path", help="è‡ªå®šä¹‰CUDAå®‰è£…è·¯å¾„")
    env_group.add_argument("--cudnn-path", help="è‡ªå®šä¹‰cuDNNåº“è·¯å¾„")
    env_group.add_argument("--ffmpeg-path", help="è‡ªå®šä¹‰FFmpegå¯æ‰§è¡Œæ–‡ä»¶ç›®å½•")
    
    # é«˜çº§å‚æ•°
    parser.add_argument("--verbose", "-v", action="store_true", help="å¯ç”¨è¯¦ç»†æ—¥å¿—")
    parser.add_argument("--keep-temp-files", action="store_true", help="ä¿ç•™ä¸´æ—¶æ–‡ä»¶ï¼ˆä¸è‡ªåŠ¨åˆ é™¤ï¼‰")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logger.setLevel(logging.DEBUG)
    
    # å¤„ç†ç¯å¢ƒå˜é‡é…ç½®
    global CUDA_PATH, CUDNN_PATH, FFMPEG_PATH
    
    # å¤„ç†CUDAè·¯å¾„
    if args.cuda_path:
        CUDA_PATH = args.cuda_path
        add_path_to_env(CUDA_PATH, "CUDA")
        # è®¾ç½®CUDA_HOMEç¯å¢ƒå˜é‡
        os.environ["CUDA_HOME"] = CUDA_PATH
        os.environ["CUDA_PATH"] = CUDA_PATH
    
    # å¤„ç†cuDNNè·¯å¾„
    if args.cudnn_path:
        CUDNN_PATH = args.cudnn_path
        add_path_to_env(CUDNN_PATH, "cuDNN")
    
    # å¤„ç†FFmpegè·¯å¾„
    if args.ffmpeg_path:
        FFMPEG_PATH = args.ffmpeg_path
        add_path_to_env(FFMPEG_PATH, "FFmpeg")
    
    # éªŒè¯è¯´è¯äººåˆ†å‰²å‚æ•°
    if args.speakers and not args.speaker_model:
        parser.error("å¯ç”¨è¯´è¯äººåˆ†å‰²æ—¶(--speakers)å¿…é¡»é€šè¿‡--speaker-modelæä¾›æ¨¡å‹è·¯å¾„æˆ–ID")
    
    return args

def process_single_file(args):
    """å¤„ç†å•ä¸ªéŸ³é¢‘æ–‡ä»¶"""
    try:
        # åˆ›å»ºè½¬å½•å™¨å®ä¾‹
        transcriber = WhisperTranscriber(
            model_path=args.model,
            device=args.device,
            compute_type=args.compute_type
        )
        
        # å¦‚æœå¯ç”¨è¯´è¯äººåˆ†å‰²
        if args.speakers:
            # éªŒè¯æ¨¡å‹è·¯å¾„å·²æä¾›
            if not args.speaker_model:
                raise ValueError("ä½¿ç”¨è¯´è¯äººåˆ†å‰²åŠŸèƒ½æ—¶å¿…é¡»æä¾›--speaker-modelå‚æ•°")
                
            # åˆ›å»ºè¯´è¯äººåˆ†å‰²å¤„ç†å™¨
            diarization = SpeakerDiarization(
                device=args.device,
                model_path=args.speaker_model
            )
            
            # æ‰§è¡Œè½¬å½•ä¸è¯´è¯äººåˆ†å‰²
            output_file = transcriber.transcribe_and_save_with_speakers(
                audio_file=args.audio,
                output_file=args.output,
                output_format=args.format,
                in_memory=args.in_memory,
                diarization=diarization,
                num_speakers=args.num_speakers,
                min_speakers=args.min_speakers,
                max_speakers=args.max_speakers,
                audio_format=args.audio_format,
                beam_size=args.beam_size,
                language=args.language,
                task=args.task,
                vad_filter=args.vad_filter,
                min_silence_ms=args.silence,
                preload_audio=args.in_memory
            )
            
            # é‡Šæ”¾èµ„æº
            diarization.close()
        else:
            # å¸¸è§„è½¬å½•
            output_file = transcriber.transcribe_and_save(
                audio_file=args.audio,
                output_file=args.output,
                output_format=args.format,
                in_memory=args.in_memory,
                beam_size=args.beam_size,
                language=args.language,
                task=args.task,
                vad_filter=args.vad_filter,
                min_silence_ms=args.silence,
                preload_audio=args.in_memory
            )
        
        # å¦‚æœæ˜¯å†…å­˜æ¨¡å¼ï¼Œéœ€è¦å†™å…¥æ–‡ä»¶
        if args.in_memory and isinstance(output_file, str) and not os.path.isfile(output_file):
            output_path = Path(args.output)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(output_file)
            
            logger.info(f"å†…å­˜å¤„ç†ç»“æœå·²ä¿å­˜è‡³: {output_path.absolute()}")
            output_file = str(output_path.absolute())
        
    except Exception as e:
        logger.error(f"è½¬å½•å¤±è´¥: {e}", exc_info=True)
        return False
    finally:
        # ç¡®ä¿èµ„æºè¢«é‡Šæ”¾
        if 'transcriber' in locals():
            transcriber.close()
    
    return True


def process_batch(args):
    """æ‰¹å¤„ç†å¤šä¸ªéŸ³é¢‘æ–‡ä»¶"""
    audio_path = Path(args.audio)
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºç›®å½•
    if not audio_path.is_dir():
        logger.error(f"æ‰¹å¤„ç†æ¨¡å¼éœ€è¦æŒ‡å®šç›®å½•: {args.audio}")
        return False
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æŸ¥æ‰¾æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
    audio_files = []
    for ext in AudioProcessor.SUPPORTED_FORMATS:
        audio_files.extend(audio_path.glob(f"*{ext}"))
        # åŒæ—¶æ£€æŸ¥å¤§å†™æ‰©å±•å
        audio_files.extend(audio_path.glob(f"*{ext.upper()}"))
    
    if not audio_files:
        logger.error(f"ç›®å½•ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„éŸ³é¢‘æ–‡ä»¶: {args.audio}")
        return False
    
    logger.info(f"æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
    
    # åˆ›å»ºè½¬å½•å™¨å®ä¾‹
    transcriber = WhisperTranscriber(
        model_path=args.model,
        device=args.device,
        compute_type=args.compute_type
    )
    
    # åˆ›å»ºè¯´è¯äººåˆ†å‰²å¤„ç†å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    diarization = None
    if args.speakers:
        # éªŒè¯æ¨¡å‹è·¯å¾„å·²æä¾›
        if not args.speaker_model:
            logger.error("ä½¿ç”¨è¯´è¯äººåˆ†å‰²åŠŸèƒ½æ—¶å¿…é¡»æä¾›--speaker-modelå‚æ•°")
            return False
            
        try:
            diarization = SpeakerDiarization(
                device=args.device,
                model_path=args.speaker_model
            )
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–è¯´è¯äººåˆ†å‰²å¤±è´¥: {e}")
            return False
    
    # æ‰¹é‡å¤„ç†
    success_count = 0
    try:
        for i, audio_file in enumerate(audio_files, 1):
            logger.info(f"å¤„ç† [{i}/{len(audio_files)}]: {audio_file.name}")
            
            # æ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
            output_file = output_dir / f"{audio_file.stem}.{args.format}"
            
            try:
                if args.speakers and diarization:
                    transcriber.transcribe_and_save_with_speakers(
                        audio_file=str(audio_file),
                        output_file=str(output_file),
                        output_format=args.format,
                        in_memory=args.in_memory,
                        diarization=diarization,
                        num_speakers=args.num_speakers,
                        min_speakers=args.min_speakers,
                        max_speakers=args.max_speakers,
                        audio_format=args.audio_format,
                        beam_size=args.beam_size,
                        language=args.language,
                        task=args.task,
                        vad_filter=args.vad_filter,
                        min_silence_ms=args.silence,
                        preload_audio=args.in_memory
                    )
                else:
                    transcriber.transcribe_and_save(
                        audio_file=str(audio_file),
                        output_file=str(output_file),
                        output_format=args.format,
                        in_memory=args.in_memory,
                        beam_size=args.beam_size,
                        language=args.language,
                        task=args.task,
                        vad_filter=args.vad_filter,
                        min_silence_ms=args.silence,
                        preload_audio=args.in_memory
                    )
                
                success_count += 1
                
            except Exception as e:
                logger.error(f"å¤„ç† {audio_file.name} å¤±è´¥: {e}")
                continue
    
    finally:
        # é‡Šæ”¾èµ„æº
        transcriber.close()
        if diarization:
            diarization.close()
    
    logger.info(f"æ‰¹å¤„ç†å®Œæˆ: æˆåŠŸ {success_count}/{len(audio_files)}")
    return success_count > 0


def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    # æ˜¾ç¤ºç¯å¢ƒä¿¡æ¯
    logger.info(f"Python ç‰ˆæœ¬: {sys.version}")
    logger.info(f"ç³»ç»Ÿå¹³å°: {sys.platform}")
    
    # æ˜¾ç¤ºä¸»è¦ç»„ä»¶è·¯å¾„
    if CUDA_PATH:
        logger.info(f"CUDA è·¯å¾„: {CUDA_PATH}")
    if CUDNN_PATH:
        logger.info(f"cuDNN è·¯å¾„: {CUDNN_PATH}")
    if FFMPEG_PATH:
        logger.info(f"FFmpeg è·¯å¾„: {FFMPEG_PATH}")
    
    # æ£€æŸ¥FFmpegæ˜¯å¦å¯ç”¨
    ffmpeg_available = AudioProcessor.check_ffmpeg_available()
    if ffmpeg_available:
        logger.info("FFmpeg å¯ç”¨: å°†ä½¿ç”¨FFmpegè¿›è¡ŒéŸ³é¢‘æ ¼å¼è½¬æ¢")
    else:
        logger.info("FFmpeg ä¸å¯ç”¨: å°†ä½¿ç”¨soundfileè¿›è¡ŒéŸ³é¢‘æ ¼å¼è½¬æ¢")
    
    # åˆ¤æ–­å¤„ç†æ¨¡å¼
    success = False
    if args.batch:
        success = process_batch(args)
    else:
        success = process_single_file(args)
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if not args.keep_temp_files:
        AudioProcessor.cleanup_temp_files()
    else:
        logger.info(f"å·²ä¿ç•™ä¸´æ—¶æ–‡ä»¶ï¼Œå…± {len(TEMP_FILES)} ä¸ªæ–‡ä»¶æœªåˆ é™¤")
    
    # æ‰§è¡Œç»Ÿè®¡
    total_time = time.time() - start_time
    logger.info(f"ç¨‹åºè¿è¡Œæ€»è€—æ—¶: {total_time:.2f}s")
    
    # è·å–GPUçŠ¶æ€
    gpu_info = SafeGPUDetector.check_cuda_available()
    if gpu_info["cuda_available"]:
        # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        memory_usage = SafeGPUDetector.get_safe_gpu_memory_usage()
        for gpu_id, info in memory_usage.items():
            logger.info(f"GPU {gpu_id} å†…å­˜å ç”¨: {info['used_GB']}/{info['total_GB']}GB "
                       f"({info['usage_percent']}%)")
    
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())